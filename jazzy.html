<!DOCTYPE html>
<html>
	<head>
		<title>WebVR Tutorial</title>
		<meta name="viewport" content="width=device-width, user-scalable=no, minimum-scale=1.0, maximum-scale=1.0, shrink-to-fit=no">

		<style>
			body {
				margin: 0;
			}

			button {
				position: absolute;
				bottom: 10px;
				right: 10px;
			}
		</style>
	</head>
	<body>

		<button id="vr">Start VR</button>

		<script src="lib/webvr-polyfill.js"></script>

		<script src="lib/three.min.js"></script>
		<script src="lib/VRControls.js"></script>
		<script src="lib/VREffect.js"></script>

		<script>

			var scene, camera, renderer;
			var controls, effect;
			var fog;
			var vrDisplay;

			var audioContext, analyser, freqDomain, source;
			var bandLower, bandUpper, bandWidth, bandVal;
			var request;
		
			var meshCount = 100;	// Number of meshes in the scene
			var lightCount = 3;
			var fieldRange = 1500; // The range in 3D space the meshes will be placed
			var meshes = []; // Array to hold meshes

			var bgColor = 0x6A2E94;
			var meshColor0 = 0x8278B6;
			var meshColor1 = 0x57FFC7;

			var material = new THREE.MeshPhongMaterial({
				color: meshColor1,
				shading: THREE.FlatShading,
				emissive: bgColor,
				specular: 0x333333,
				shininess: 70
			});

			loadAudio();

			function loadAudio() {

				audioContext = new( window.AudioContext || window.webkitAudioContext );

				request = new XMLHttpRequest();
			    request.open('GET', 'assets/loop.mp3', true);
			    request.responseType = 'arraybuffer';
			    request.onload = function() {
			        audioContext.decodeAudioData(request.response, onBufferload);
			    };
			    request.send();

			}

			function onBufferload(buffer) {

				
			    analyser = audioContext.createAnalyser();
			    freqDomain = new Uint8Array( analyser.frequencyBinCount );

			    // Easy way to grab a band of frequencies
			    bandLower = Math.floor(analyser.frequencyBinCount * 0.4);
			    bandUpper = Math.floor(analyser.frequencyBinCount * 0.6);
			    bandWidth = bandUpper - bandLower;

				source = audioContext.createBufferSource();

	   			// Connect graph
	   			source.connect(analyser);

				//pipe to speakers
				analyser.connect( audioContext.destination );
				  
				source.buffer = buffer;
				source.loop = true;

				source.start(0);

				init();

			}

			function init() {

				// This is your standard three.js set up
				// Create a scene, camera and a renderer
			    scene = new THREE.Scene();
			    camera = new THREE.PerspectiveCamera( 75, window.innerWidth / window.innerHeight, 1, 10000 );
			    renderer = new THREE.WebGLRenderer();
			    renderer.setSize( window.innerWidth, window.innerHeight );
			    document.body.appendChild( renderer.domElement );

			    // Set VR controls
			    controls = new THREE.VRControls( camera );

			    // Apply VR stereo rendering to renderer.
				effect = new THREE.VREffect(renderer);
				effect.setSize(window.innerWidth, window.innerHeight);

				navigator.getVRDisplays().then(function(displays) {
  					if (displays.length > 0) {
					    vrDisplay = displays[0];
					  }
				});

			    // Set the background and fog just for fun
			    scene.background = new THREE.Color( bgColor );
			    scene.fog = new THREE.FogExp2( bgColor, 0.002 )


			    light = new THREE.AmbientLight(bgColor, 0.012);
			    scene.add(light);

			    for (var i = 0; i < lightCount; i++) {
			    	light = new THREE.PointLight(bgColor, 0.7);
			    	randomPosition(light);
			    	scene.add(light);
			    }

			    // Create a bunch of meshes
			    for (var i = 0; i < meshCount; i++) {
			    	meshes.push(new Mesh());
			    }

			    animate();

			}

			function randomPosition(mesh) {
				mesh.position.x = (Math.random() * fieldRange) - fieldRange/2;
				mesh.position.y = (Math.random() * fieldRange) - fieldRange/2;
				mesh.position.z = (Math.random() * fieldRange) - fieldRange/2;
			}

			// mesh object adds a mesh randomly in space 
			function Mesh() {

				// Standard three.js to create a mesh
				// Each mesh needs a material and geometry which is used to create a mesh
			
				var geometry = new THREE.IcosahedronBufferGeometry( 50 );
				var container = new THREE.Object3D();
				var mesh = new THREE.Mesh( geometry, material );

				var orbitSpeedX = 0.01 * Math.random() - 0.005;
				var orbitSpeedY = 0.01 * Math.random() - 0.005;

				var scale;

				// Give each mesh a random position
				randomPosition(mesh);

				// Add mesh to scene
				container.add(mesh);
				scene.add(container);

				// Rotate the mesh a bit
				this.update = function() {

					container.rotation.x += orbitSpeedX;
					container.rotation.y += orbitSpeedY;

					mesh.rotation.x += 0.01;
					mesh.rotation.y += 0.02;

					scale = 1 + (bandVal*2);
					mesh.scale.set(scale, scale, scale);

				}

			}

			// Get an average of the audio band
			function updateAudio() {
				
				analyser.getByteFrequencyData(freqDomain);

				var sum = 0;

				for (var i = bandLower; i < bandUpper; i++) {
					sum += freqDomain[i];
				}

				bandVal = (sum / bandWidth) / 256;

			}



			function onResize() {
			  effect.setSize(window.innerWidth, window.innerHeight);
			  camera.aspect = window.innerWidth / window.innerHeight;
			  camera.updateProjectionMatrix();
			}

			// Standard way to run an animation in JS
			// Using requestAnimationFrame
			function animate() {

			    requestAnimationFrame( animate );

			    updateAudio();

			    // Every frame, animate the meshes with the update method
			    for (var i = 0; i < meshCount; i++) {
			    	meshes[i].update();
			    }

			    // Controls need updating every frame
			    controls.update();

			    // Render the scene
			    effect.render( scene, camera );

			}


			document.querySelector('#vr').addEventListener('click', function() {
			  vrDisplay.requestPresent([{source: renderer.domElement}]);
			});

			window.addEventListener('resize', onResize);
			window.addEventListener('vrdisplaypresentchange', onResize);


		</script>

	</body>

</html>